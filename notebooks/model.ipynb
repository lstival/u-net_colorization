{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mjy8086/semantic_segmentation_2D_ViT_UNet/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda Devices avalibles\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ViT_UNet is consists of 3 parts\n",
    "# 1) CNN Encoder\n",
    "# 2) bottle neck Vision Trnasformer(ViT)\n",
    "# 3) CNN Decoder\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Making ViT\n",
    "\n",
    "# # Patch Embedding\n",
    "\n",
    "# class Embeddings(nn.Module):\n",
    "#     \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, img_size):\n",
    "#         super(Embeddings, self).__init__()\n",
    "#         down_factor = 4\n",
    "#         # input image가 얼마나 많이 pooling을 거치냐가 down_factor\n",
    "#         # Maxpool2d가 4번 있으니 down_factor = 4\n",
    "#         patch_size = (2, 2)\n",
    "#         # patch_size는 2로 설정\n",
    "#         n_patches = int((img_size[0]/2**down_factor// patch_size[0]) * (img_size[1]/2**down_factor// patch_size[1]))\n",
    "#         # n_pathces = (512/2**4//8) * (768/2**4//8) = 4\n",
    "#         self.patch_embeddings = Conv2d(in_channels=256,\n",
    "#                                        # 우선 in channels는 128로 설정하자\n",
    "#                                        out_channels=768,\n",
    "#                                        # out_channels = hidden size D = 768\n",
    "#                                        kernel_size=patch_size,\n",
    "#                                        stride=patch_size)\n",
    "#         self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, 768))\n",
    "\n",
    "#         self.dropout = Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # input = (B, 256, 48, 32)\n",
    "#         x = self.patch_embeddings(x)  # (B, hidden, n_patches^(1/2), n_patches^(1/2))\n",
    "#         # (B, 768, 24, 16)\n",
    "#         x = x.flatten(2)\n",
    "#         # (B, 768, 384)\n",
    "#         x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n",
    "#         # (B, 384, 768)\n",
    "#         position_embeddings = self.position_embeddings\n",
    "#         # position_embeddings = (B, 384, 768)\n",
    "#         embeddings = x + position_embeddings\n",
    "#         # (B, 384, 768)\n",
    "#         embeddings = self.dropout(embeddings)\n",
    "#         return embeddings\n",
    "\n",
    "\n",
    "# # Multi-head self attention (MSA) - layer norm not included\n",
    "\n",
    "# class MSA(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MSA, self).__init__()\n",
    "#         self.num_attention_heads = 12\n",
    "#         # Number of head = 12\n",
    "#         self.attention_head_size = int(768 / self.num_attention_heads)\n",
    "#         # Attention Head size = Hidden size(D)(768) / Number of head(12) = 64\n",
    "#         self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "#         # All Head size = (12 * 64) = 768 = Hidden size\n",
    "#         self.query = Linear(768, self.all_head_size)\n",
    "#         self.key = Linear(768, self.all_head_size)\n",
    "#         self.value = Linear(768, self.all_head_size)\n",
    "#         self.out = Linear(768, 768)\n",
    "#         self.attn_dropout = Dropout(0.1)\n",
    "#         self.proj_dropout = Dropout(0.1)\n",
    "#         self.softmax = Softmax(dim=-1)\n",
    "\n",
    "#     def transpose_for_scores(self, x):\n",
    "#         x = x.view([x.size()[0], -1, self.num_attention_heads, self.attention_head_size])\n",
    "#         return x.permute(0, 2, 1, 3)\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         mixed_query_layer = self.query(hidden_states)\n",
    "#         mixed_key_layer = self.key(hidden_states)\n",
    "#         mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "#         query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "#         key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "#         value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "#         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "#         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "#         attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "#         attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "#         context_layer = torch.matmul(attention_probs, value_layer)\n",
    "#         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "#         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "#         context_layer = context_layer.view(*new_context_layer_shape)\n",
    "#         attention_output = self.out(context_layer)\n",
    "#         attention_output = self.proj_dropout(attention_output)\n",
    "#         return attention_output\n",
    "\n",
    "\n",
    "# # MLP - layer norm not included\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = Linear(768, 3072)\n",
    "#         self.fc2 = Linear(3072, 768)\n",
    "#         self.act_fn = torch.nn.functional.gelu\n",
    "#         self.dropout = Dropout(0.1)\n",
    "#         self._init_weights()\n",
    "\n",
    "#     def _init_weights(self):\n",
    "#         nn.init.xavier_uniform_(self.fc1.weight)\n",
    "#         nn.init.xavier_uniform_(self.fc2.weight)\n",
    "#         nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "#         nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.act_fn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Block - incorporating MSA, MLP, Layer Norm\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Block, self).__init__()\n",
    "#         self.hidden_size = 768\n",
    "#         self.attention_norm = LayerNorm(768, eps=1e-6)\n",
    "#         self.ffn_norm = LayerNorm(768, eps=1e-6)\n",
    "#         self.ffn = MLP()\n",
    "#         self.attn = MSA()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = x\n",
    "\n",
    "#         x = self.attention_norm(x)\n",
    "#         x = self.attn(x)\n",
    "#         x = x + h\n",
    "\n",
    "#         h = x\n",
    "#         x = self.ffn_norm(x)\n",
    "#         x = self.ffn(x)\n",
    "#         x = x + h\n",
    "#         return x\n",
    "\n",
    "\n",
    "# #  ViTencoder - ViT Encoder with Blocks\n",
    "\n",
    "# class ViTencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ViTencoder, self).__init__()\n",
    "#         self.layer = nn.ModuleList()\n",
    "#         self.encoder_norm = LayerNorm(768, eps=1e-6)\n",
    "#         for _ in range(12):\n",
    "#             layer = Block()\n",
    "#             self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "\n",
    "\n",
    "#         for layer_block in self.layer:\n",
    "#             hidden_states = layer_block(hidden_states)\n",
    "\n",
    "#         encoded = self.encoder_norm(hidden_states)\n",
    "#         return encoded\n",
    "\n",
    "\n",
    "# #  ViT 마지막에 나온 latent를 CNNdecoder에 넣기 위해 변환시키기위한 Conv\n",
    "\n",
    "# class Conv2dReLU(nn.Sequential):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             in_channels,\n",
    "#             out_channels,\n",
    "#             kernel_size,\n",
    "#             padding=0,\n",
    "#             stride=1,\n",
    "#             use_groupnorm=True,\n",
    "#     ):\n",
    "#         conv = nn.Conv2d(\n",
    "#             in_channels,\n",
    "#             out_channels,\n",
    "#             kernel_size,\n",
    "#             stride=stride,\n",
    "#             padding=padding,\n",
    "#             bias=not (use_groupnorm),\n",
    "#         )\n",
    "#         relu = nn.LeakyReLU(inplace=True)\n",
    "\n",
    "#         gn = nn.GroupNorm(16, out_channels, eps=1e-6)\n",
    "\n",
    "#         super(Conv2dReLU, self).__init__(conv, gn, relu)\n",
    "\n",
    "\n",
    "# #  ViT\n",
    "\n",
    "# class ViT(nn.Module):\n",
    "#     def __init__(self, img_size):\n",
    "#         super(ViT, self).__init__()\n",
    "#         self.embeddings = Embeddings(img_size=img_size)\n",
    "#         self.encoder = ViTencoder()\n",
    "#         self.img_size = img_size\n",
    "#         self.patch_size = (2, 2)\n",
    "#         self.down_factor = 4\n",
    "#         self.conv_more = Conv2dReLU(768, 256, kernel_size=3, padding=1, use_groupnorm=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # (B, 256, 32, 48)\n",
    "#         x = self.embeddings(x)\n",
    "#         # (B, 384, 768)\n",
    "#         x = self.encoder(x)  # (B, n_patch, hidden)\n",
    "#         # (B, 384, 768)\n",
    "#         B, n_patch, hidden = x.size()\n",
    "#         # B=B, n_patch=384, hidden=768\n",
    "#         h, w = (self.img_size[0] // 2**self.down_factor // self.patch_size[0]), (self.img_size[1] // 2**self.down_factor // self.patch_size[0])\n",
    "#         # h=24, w=16\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         # (B, 768, 384)\n",
    "#         x = x.contiguous().view(B, hidden, h, w)\n",
    "#         # (B, 768, 16, 24)\n",
    "#         x = self.conv_more(x)\n",
    "#         # (B, 256, 16, 24)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# # Generator\n",
    "\n",
    "# # CNN Encoder - ViT bottle neck - CNN Decoder\n",
    "# class ViT_UNet(nn.Module):\n",
    "#     def __init__(self, img_size=(512, 768)):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "#         self.conv1_1 = CNNencoder_gn(3, 16)\n",
    "#         self.conv1_2 = CNNencoder_gn(16, 16)\n",
    "#         self.conv2_1 = CNNencoder_gn(16, 32)\n",
    "#         self.conv2_2 = CNNencoder_gn(32, 32)\n",
    "#         self.conv3_1 = CNNencoder_gn(32, 64)\n",
    "#         self.conv3_2 = CNNencoder_gn(64, 64)\n",
    "#         self.conv4_1 = CNNencoder_gn(64, 128)\n",
    "#         self.conv4_2 = CNNencoder_gn(128, 128)\n",
    "#         self.conv5_1 = CNNencoder_gn(128, 256)\n",
    "#         self.conv5_2 = CNNencoder_gn(256, 256)\n",
    "\n",
    "#         self.vit = ViT(img_size)\n",
    "\n",
    "#         self.concat1 = Concat_gn(512, 128)\n",
    "#         self.convup1 = CNNencoder_gn(128, 128)\n",
    "#         self.concat2 = Concat_gn(256, 64)\n",
    "#         self.convup2 = CNNencoder_gn(64, 64)\n",
    "#         self.concat3 = Concat_gn(128, 32)\n",
    "#         self.convup3 = CNNencoder_gn(32, 32)\n",
    "#         self.concat4 = Concat_gn(64, 16)\n",
    "#         self.convup4 = CNNencoder_gn(16, 16)\n",
    "#         self.concat5 = Concat_ln(32, 23)\n",
    "#         self.convup5 = CNNencoder_ln(23, 23)\n",
    "\n",
    "#         self.Segmentation_head = nn.Conv2d(23, 23, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # (B, in_channel, 512, 768)\n",
    "#         c1 = self.conv1_1(x)\n",
    "#         c1 = self.conv1_2(c1)\n",
    "#         # (B, 16, 512, 768)\n",
    "#         p1 = self.pooling(c1)\n",
    "#         # (B, 16, 256, 384)\n",
    "#         c2 = self.conv2_1(p1)\n",
    "#         c2 = self.conv2_2(c2)\n",
    "#         # (B, 16, 256, 384)\n",
    "#         p2 = self.pooling(c2)\n",
    "#         # (B, 32, 128, 192)\n",
    "#         c3 = self.conv3_1(p2)\n",
    "#         c3 = self.conv3_2(c3)\n",
    "#         # (B, 32, 128, 192)\n",
    "#         p3 = self.pooling(c3)\n",
    "#         # (B, 64, 64, 96)\n",
    "#         c4 = self.conv4_1(p3)\n",
    "#         c4 = self.conv4_2(c4)\n",
    "#         # (B, 128, 64, 96)\n",
    "#         p4 = self.pooling(c4)\n",
    "#         # (B, 128, 32, 48)\n",
    "#         c5 = self.conv5_1(p4)\n",
    "#         c5 = self.conv5_2(c5)\n",
    "#         # (B, 256, 32, 48)\n",
    "#         v = self.vit(c5)\n",
    "#         # (B, 256, 16, 24)\n",
    "#         v1 = self.upsample(v)\n",
    "#         # (B, 256, 32, 48)\n",
    "#         u1 = self.concat1(v1, c5)\n",
    "#         u1 = self.convup1(u1)\n",
    "#         # (B, 128, 32, 48)\n",
    "#         u1 = self.upsample(u1)\n",
    "#         # (B, 128, 64, 96)\n",
    "#         u2 = self.concat2(u1, c4)\n",
    "#         u2 = self.convup2(u2)\n",
    "#         # (B, 64, 64, 96)\n",
    "#         u2 = self.upsample(u2)\n",
    "#         # (B, 64, 128, 192)\n",
    "#         u3 = self.concat3(u2, c3)\n",
    "#         u3 = self.convup3(u3)\n",
    "#         # (B, 32, 128, 192)\n",
    "#         u3 = self.upsample(u3)\n",
    "#         # (B, 32, 256, 384)\n",
    "#         u4 = self.concat4(u3, c2)\n",
    "#         u4 = self.convup4(u4)\n",
    "#         # (B, 16, 256, 384)\n",
    "#         u4 = self.upsample(u4)\n",
    "#         # (B, 16, 512, 768)\n",
    "#         u5 = self.concat5(u4, c1)\n",
    "#         u5 = self.convup5(u5)\n",
    "#         # (B, 23, 512, 768)\n",
    "#         out = self.Segmentation_head(u5)\n",
    "#         # (B, 23, 512, 768)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c8146f26135f1c926afd56a5c89149aa7350eae42937c6df54039f048d566a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
